{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib2\n",
    "import bs4 as bs\n",
    "from bs4 import SoupStrainer\n",
    "import numpy as np\n",
    "df = pd.read_excel('health.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "sicf=df['SIC_final'].values\n",
    "cik=[]\n",
    "link=[]\n",
    "comp=[]\n",
    "sic2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8011L,\n",
       " 8021L,\n",
       " 8031L,\n",
       " 8041L,\n",
       " 8042L,\n",
       " 8043L,\n",
       " 8049L,\n",
       " 8051L,\n",
       " 8052L,\n",
       " 8059L,\n",
       " 8062L,\n",
       " 8063L,\n",
       " 8069L,\n",
       " 8071L,\n",
       " 8072L,\n",
       " 8082L,\n",
       " 8092L,\n",
       " 8093L,\n",
       " 8099L]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sicf.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def webdata(url,s):\n",
    "    a=[]\n",
    "    html = urllib2.urlopen(url).read()\n",
    "    if(html.find(\"Companies for \")>0):\n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        for c in soup.find_all('a', href=True, text=True, id=''):\n",
    "            if \"getcompany&CIK\" in c['href']:\n",
    "                link.append('https://www.sec.gov'+c['href'])\n",
    "        rows = soup.find_all('tr')   #Searching for all <tr>\n",
    "        for row in rows:                   \n",
    "            td_cells = row.find_all('td')   #Within tr searching for td\n",
    "            for td_cell in td_cells:\n",
    "                a.append(td_cell.text)\n",
    "        i=0\n",
    "        while i<len(a):\n",
    "            cik.append(a[i])\n",
    "            comp.append(a[i+1])\n",
    "            sic2.append(s)\n",
    "            i=i+3\n",
    "        if(soup.find(\"input\", value=True)):\n",
    "            if(soup.find(\"input\", value=True)['value']=='Previous 100'):\n",
    "                if(len(soup.find_all(\"input\", value=True))>1):\n",
    "                    html2=str(soup.find_all(\"input\", value=True)[1])\n",
    "                    soup1 = BeautifulSoup(html2,'lxml')\n",
    "                    x= soup1.find(\"input\", value=True)[\"onclick\"]\n",
    "                    url_temp='https://www.sec.gov'+ x[17:len(x)-1]\n",
    "                    webdata(url_temp,s)\n",
    "            else:\n",
    "                x= soup.find(\"input\", value=True)[\"onclick\"]\n",
    "                url_temp='https://www.sec.gov'+ x[17:len(x)-1]\n",
    "                webdata(url_temp,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in sicf:\n",
    "    url='https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&SIC=' + str(k) + '&owner=exclude&match=&start=0&count=100&hidefilings=0'\n",
    "    webdata(url,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputdf=pd.DataFrame(comp, columns=['Company Name'])\n",
    "outputdf['CIK']=cik\n",
    "outputdf['SIC']= sic2\n",
    "outputdf['weblink']= link\n",
    "#outputdf.to_csv('healthcik.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Users\\Lenovo\\Anaconda2\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "company=comp\n",
    "final_url=[]\n",
    "date=[]\n",
    "type_form=[] \n",
    "company_name=[]\n",
    "url_search=[]\n",
    "def datasort(html,m):\n",
    "    a=[]\n",
    "    soup = BeautifulSoup(html)    #Converting html to soup object\n",
    "    rows = soup.find_all('tr')   #Searching for all <tr>\n",
    "    for row in rows:                   \n",
    "        td_cells = row.find_all('td')   #Within tr searching for td\n",
    "        for td_cell in td_cells:\n",
    "            a.append(td_cell.text)    #Within td appending all the text in a list\n",
    "    k=0\n",
    "    while k<len(a):\n",
    "        if \"Annual report\" in a[k] or \"Transition reports\" in a[k] :   #Searching for files\n",
    "            type_form.append(a[k-2])  #Appending filing type and date based on the above criteria\n",
    "            date.append(a[k+1])\n",
    "        k=k+1\n",
    "    for c in soup.find_all('a', href=True, text=True, id='documentsbutton'):\n",
    "        if \"/Archives/edgar/data/\" in c['href']:   #Looking for links\n",
    "            final_url.append(c['href'])\n",
    "            company_name.append(m)\n",
    "company_temp1=[]\n",
    "for c in company:\n",
    "    company_temp=c\n",
    "    c=c.replace(\"&\",\"%26\")\n",
    "    c=c.replace(\" \",\"+\")\n",
    "    url=\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&company=\"+c+\"&type=10-K&dateb=&owner=exclude&count=10000\"\n",
    "    html = urllib2.urlopen(url).read()\n",
    "    if(html.find(\"Companies with names matching\")>0):        #Identfying companies for which we dont get the desired page directly\n",
    "        while html.find('<a href=\"/cgi-bin/browse-edgar?action=getcompany&amp;CIK=') !=-1:\n",
    "            start_temp = html.find('<a href=\"/cgi-bin/browse-edgar?action=getcompany&amp;CIK=')\n",
    "            end_temp = html.find('<acronym title=\"Standard Industrial Code\">SIC</acronym>')\n",
    "            url_temp=html[start_temp+57:start_temp+67]\n",
    "            url_search.append('https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK='+url_temp+'&owner=exclude&count=10000&hidefilings=0&type=10-K')\n",
    "            company_temp1.append(company_temp)\n",
    "            html=html[end_temp+200:]\n",
    "    elif(html.find(\"Companies with names matching\") <1 and html.find(\"No matching companies\") <1):\n",
    "        datasort(html,company_temp)\n",
    "n=0\n",
    "for u in url_search:\n",
    "    html = urllib2.urlopen(u).read()\n",
    "    datasort(html,company_temp1[n])\n",
    "    n=n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputdf=pd.DataFrame(company_name, columns=['Company Name'])\n",
    "outputdf['Date of Filing']=date\n",
    "outputdf['Filing Type']= type_form\n",
    "outputdf['weblink']= final_url\n",
    "\n",
    "outputdf=outputdf[outputdf['Filing Type']=='10-K']\n",
    "outputdf.to_csv('10K_Links_health.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
